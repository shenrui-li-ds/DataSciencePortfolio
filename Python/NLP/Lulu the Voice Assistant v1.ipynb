{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09603fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import weather_bot\n",
    "import twitter_bot\n",
    "import location_bot\n",
    "import news_bot\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import json\n",
    "import string\n",
    "import random \n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabf51e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_weather = weather_bot.current_weather()\n",
    "detail_weather = weather_bot.detail_weather_forecast()\n",
    "tomorrow_weather = detail_weather['daily'][1]\n",
    "next_hour_weather = detail_weather['hourly'][1]\n",
    "local_time = datetime.now()\n",
    "date_now = local_time.strftime('%Y-%m-%d %A')\n",
    "time_now = local_time.strftime('%H:%M:%S %p')\n",
    "schedule = \"You have no scheduled activity.\"\n",
    "trends, top10_trend_str = twitter_bot.get_trend_in_my_loc()\n",
    "most_popular_tweet = twitter_bot.get_most_popular_tweet()\n",
    "twitter_first_tweet = twitter_bot.get_first_tweet_on_timeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e2c54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yake\n",
    "def keyword_extractor(sentence):\n",
    "    kw_extractor = yake.KeywordExtractor()\n",
    "    language = \"en\"\n",
    "    max_ngram_size = 3\n",
    "    deduplication_threshold = 0.1\n",
    "    numOfKeywords = 10\n",
    "    custom_kw_extractor = \\\n",
    "    yake.KeywordExtractor(lan=language, n=max_ngram_size, \n",
    "                          dedupLim=deduplication_threshold, \n",
    "                          top=numOfKeywords, features=None)\n",
    "    keywords = custom_kw_extractor.extract_keywords(sentence)\n",
    "    keywords.sort(key=lambda x: x[1], reverse=True)\n",
    "    # return the highest possible keyword\n",
    "    return keywords[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from bs4 import BeautifulSoup\n",
    "def get_google_search(query):\n",
    "#     query = \"quantum physics\"\n",
    "    raw = get(f\"https://www.google.com/search?q={query}\")\n",
    "    soup = BeautifulSoup(raw.text, 'html')\n",
    "    description = soup.find_all(\"div\",{\"class\":\"BNeawe s3v9rd AP7Wnd\"})\n",
    "    # return the text of the first result\n",
    "    return description[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17571e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "def get_google_1result(query):\n",
    "#     query = \"samira\"\n",
    "    url_list = [*search(query, tld=\"co.in\", num=5, stop=5, pause=2)]\n",
    "    # return the first url\n",
    "    print(url_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c06d0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kelvinToCelsius(kelvin):\n",
    "    return kelvin - 273.15\n",
    "\n",
    "def precip_likely(precip):\n",
    "    if precip < 10:\n",
    "        return \"Precipitation is very unlikely.\"\n",
    "    elif (precip >= 10)&(precip < 35):\n",
    "        return \"Precipitation is unlikely.\"\n",
    "    elif (precip >= 35)&(precip < 65):\n",
    "        return \"It's likely to have some precipitation.\"\n",
    "    elif (precip >= 65)&(precip < 90):\n",
    "        return \"It's very likely to have some precipitation in the next hour.\"\n",
    "    else:\n",
    "        return \"It's most likely to have precipitation in the next hour.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0624ff45",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_weather_str = \\\n",
    "f\"The weather right now is {current_weather['weather'][0]['description']}\"+\\\n",
    "f\"\\nwith temperature of {kelvinToCelsius(current_weather['main']['temp']):.0f} degree celsius\"+\\\n",
    "f\"\\nand feels like: {kelvinToCelsius(current_weather['main']['feels_like']):.0f} degree celsius.\"+\\\n",
    "f\"\\nHumidity is {current_weather['main']['humidity']:.1f} percents\"\n",
    "\n",
    "tomorrow_weather_str = \\\n",
    "f\"The weather tomorrow will be {next_hour_weather['weather'][0]['description']}\"+\\\n",
    "f\"\\nwith temperature of {kelvinToCelsius(tomorrow_weather['temp']['day']):.0f} degree celsius during the day\"+\\\n",
    "f\"\\nand {kelvinToCelsius(tomorrow_weather['temp']['night']):.0f} degree celsius during the night.\"+\\\n",
    "f\"\\nFeels like {kelvinToCelsius(tomorrow_weather['feels_like']['day']):.0f} degree celsius during the day\"+\\\n",
    "f\"\\nand {kelvinToCelsius(tomorrow_weather['feels_like']['night']):.0f} degree celsius during the night\"+\\\n",
    "\"\\nSunrise will be \"+(datetime.utcfromtimestamp(tomorrow_weather['sunrise'])+\n",
    "              timedelta(hours=-5)).strftime('%H:%M')+\" in the morning, \"+\\\n",
    "\"\\nand sunset will be \"+(datetime.utcfromtimestamp(tomorrow_weather['sunset'])+\n",
    "              timedelta(hours=-5)).strftime('%H:%M')+\" in the afternoon.\"\n",
    "\n",
    "next_hour_weather_str = \\\n",
    "f\"The weather will be {next_hour_weather['weather'][0]['description']} in the next hour, \"+\\\n",
    "f\"\\nwith temperature of {kelvinToCelsius(next_hour_weather['temp']):.0f} degree celsius\"+\\\n",
    "f\"\\nand humidity of {next_hour_weather['humidity']:.1f} percents.\"+\\\n",
    "f\"\\nUV Index is {next_hour_weather['uvi']:.1f}\"+\\\n",
    "\"\\n\"+precip_likely(detail_weather['minutely'][0]['precipitation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_list = [\n",
    "    {\n",
    "        \"tag\": \"Greeting\",\n",
    "        \"patterns\": [\"Hello\", \"How are you?\", \"Hi there\", \"Hi\", \"Whats up\",\"good morning\",\"good afternoon\",\n",
    "                     \"good evening\", \"How's it going\",\"How's everything\",\"Howdy\"],\n",
    "        \"responses\": [\"Howdy\", \"Hello\", \"How are you doing?\", \"Greetings!\", \"How do you do?\"]\n",
    "     }]\n",
    "\n",
    "bye_list = [\n",
    "    {\n",
    "        \"tag\": \"Goodbye\",\n",
    "        \"patterns\": [\"bye\", \"got to go\", \"see ya\", \"adios\", \"see you soon\",\"end conversation\",\"later\",\n",
    "                     \"I gotta go\",\"have a good one\",\"have a nice day\",\"enjoy your evening\",\n",
    "                    \"enjoy the rest of your day\",\"good night\"],\n",
    "        \"responses\": [\"It was nice speaking to you.\", \"We'll be in contact soon.\", \"Speak soon!\",\n",
    "                     \"Keep in touch!\"]\n",
    "     }]\n",
    "\n",
    "search_list = [\n",
    "    {\n",
    "        \"tag\": \"GoogleSearch\",\n",
    "        \"patterns\": [\"How does mumbo-jumbo work?\", \"can you explalin mumbo-jumbo to me?\",\n",
    "                    \"can you show me mumbo-jumbo on google?\", \"who is mumbo-jumbo\",\n",
    "                    \"show me how mumbo-jumbo works\",\"what is mumbo-jumbo\",\n",
    "                    \"tell me about mumbo-jumbo\",\"tell me how mumbo-jumbo works\",\n",
    "                    \"search something on google\",\"something recommendation in somewhere\",\n",
    "                    \"show me things to do in somewhere\",\"show me places to go in somewhere\",\n",
    "                    \"show me restaurants in somewhere\",\"show me something in somewhere\"],\n",
    "        \"responses\": [\"\"]\n",
    "     },{\n",
    "        \"tag\": \"MapsSearch\",\n",
    "        \"patterns\": [\"what is the opening hour of mumbo-jumbo\",\"where is mumbo-jumbo\",\n",
    "                    \"show me something on the map\"],\n",
    "        \"responses\": [\"\"]\n",
    "     },{\n",
    "        \"tag\": \"NewsSearch\",\n",
    "        \"patterns\": [\"what is happening in mumbo-jumbo\",\"news story about mumbo-jumbo\",\n",
    "                    \"what happened in somewhere\",\"show me the news in somewhere\"],\n",
    "        \"responses\": [\"\"]\n",
    "     }]\n",
    "\n",
    "weather_list = [\n",
    "    {\n",
    "        \"tag\": \"Current\",\n",
    "        \"patterns\": [ \"what's the weather like today\",\"what's the weather outside\",\"what's the current weather\",\n",
    "                    \"how's the weather\",\"how's the weather like\",\"is it cold outside\",\"is it hot today\"],\n",
    "        \"responses\": [current_weather_str]\n",
    "    },{\n",
    "        \"tag\": \"Tomorrow\",\n",
    "        \"patterns\": [ \"what's the weather like tomorrow\",\"how's the weather tomorrow\",\n",
    "                     \"what's the weather like the coming day\"],\n",
    "        \"responses\": [tomorrow_weather_str]\n",
    "    },{\n",
    "        \"tag\": \"Next Hour\",\n",
    "        \"patterns\": [ \"what's the weather like in the next hour\",\"is it going to rain\",\n",
    "                     \"what's the weather like in the coming hour\"],\n",
    "        \"responses\": [next_hour_weather_str]\n",
    "    }]\n",
    "\n",
    "datetime_list = [\n",
    "    {\n",
    "        \"tag\": \"Date\",\n",
    "        \"patterns\": [ \"what day is it today\",\"what's the date today\"],\n",
    "        \"responses\": [\"Today is \"+date_now]\n",
    "    },{\n",
    "        \"tag\": \"Time\",\n",
    "        \"patterns\": [ \"what time is it\",\"what time is it now\"],\n",
    "        \"responses\": [\"The time is now \"+time_now]\n",
    "    },{\n",
    "        \"tag\": \"Calendar\",\n",
    "        \"patterns\": [ \"what's on my calendar\",\"what is my schedule\",\"what's on my schedule\"],\n",
    "        \"responses\": [schedule]\n",
    "    }]\n",
    "\n",
    "ai_list = [\n",
    "    {\n",
    "        \"tag\": \"Name\",\n",
    "        \"patterns\": [\"who are you\",\"what's your name\",\"what should I call you\",\"introduce yourself\",\n",
    "                     \"tell me about yourself\"],\n",
    "        \"responses\": [\"My name is Lulu, your personal debug companion:)\",\n",
    "                     \"I'm Lulu, I'm here to solve your debug problems.\"]\n",
    "    },{\n",
    "        \"tag\": \"Age\",\n",
    "        \"patterns\": [ \"how old are you\",\"when are you born\",\"when are you made\"],\n",
    "        \"responses\": [\"I'm 1-day-old, I just born yesterday.\"]\n",
    "    }]\n",
    "\n",
    "twitter_list = [\n",
    "    {\n",
    "        \"tag\": \"Trending\",\n",
    "        \"patterns\": [ \"what's trending right now\",\"what's popular on twitter\",\n",
    "                     \"what's the hottest topic\",\"show me the trending topics on twitter\",\n",
    "                     \"what's on twitter\"],\n",
    "        \"responses\": [top10_trend_str]\n",
    "    },{\n",
    "        \"tag\": \"First Tweet\",\n",
    "        \"patterns\": [ \"what's the first tweet on my timeline\",\"what's on my timeline\",\n",
    "                    \"my home timeline\"],\n",
    "        \"responses\": [twitter_first_tweet]\n",
    "    },{\n",
    "        \"tag\": \"Most Popular Tweet\",\n",
    "        \"patterns\": [ \"what's the most popular tweet\",\"what's the trending tweet\"],\n",
    "        \"responses\": [most_popular_tweet]\n",
    "    }]\n",
    "\n",
    "jokes_list = [\n",
    "    {\n",
    "        \"tag\": \"General\",\n",
    "        \"patterns\": [ \"tell me a joke\",\"tell me something funny\"],\n",
    "        \"responses\": [\"What do you call a fish wearing a bowtie? Sofishticated.\",\n",
    "                     \"Dear Math, grow up and solve your own problems.\",\n",
    "                     \"I only know 25 letters of the alphabet. I don't know Y.\",\n",
    "                     \"Why does Teemo live in a small house? He doesn't need mushroom.\",\n",
    "                     \"Why did the manaless Syndra run from the teamfight? She didnâ€™t have the balls.\",\n",
    "                     \"How does Janna shield her allies? With Ease.\",\n",
    "                     \"Whats vayne's favorite website? tumblr.\"]\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d1871",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Greeting\":greet_list,\n",
    "    \"Goodbye\":bye_list,\n",
    "    \"Search\":search_list,\n",
    "    \"WeatherInfo\":weather_list,\n",
    "    \"TwitterInfo\":twitter_list,\n",
    "    \"AIInfo\":ai_list,\n",
    "    \"DateTimeInfo\":datetime_list,\n",
    "    \"Jokes\":jokes_list\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdc6232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing lemmatizer to get stem of words\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Each list to create\n",
    "words = []\n",
    "classes_outer = []\n",
    "classes_inner = []\n",
    "x_patterns = []\n",
    "y_inner_intents = []\n",
    "y_outer_intents = []\n",
    "# Loop through all the intents\n",
    "# tokenize each pattern and append tokens to words, the patterns and\n",
    "# the associated tag to their associated list\n",
    "for outer_intent in data.keys():\n",
    "    for inner_intent in data[outer_intent]:\n",
    "        for pattern in inner_intent[\"patterns\"]:\n",
    "            tokens = nltk.word_tokenize(pattern)\n",
    "            words.extend(tokens)\n",
    "            x_patterns.append(pattern)\n",
    "            y_inner_intents.append(inner_intent[\"tag\"])\n",
    "            y_outer_intents.append(outer_intent)\n",
    "        \n",
    "        if inner_intent[\"tag\"] not in classes_inner:\n",
    "            classes_inner.append(inner_intent[\"tag\"])\n",
    "    # add the tag to the classes if it's not there already \n",
    "    if outer_intent not in classes_outer:\n",
    "        classes_outer.append(outer_intent)\n",
    "# lemmatize all the words in the vocab and convert them to lowercase\n",
    "# if the words don't appear in punctuation\n",
    "words = [lemmatizer.lemmatize(word.lower()) for word in words if word not in string.punctuation]\n",
    "# sorting the vocab and classes in alphabetical order and taking the # set to ensure no duplicates occur\n",
    "words = sorted(set(words))\n",
    "classes_outer = sorted(set(classes_outer))\n",
    "classes_inner = sorted(set(classes_inner))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1176e1a",
   "metadata": {},
   "source": [
    "#### Inner Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d256c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for training data\n",
    "training = []\n",
    "out_empty = [0] * len(classes_inner)\n",
    "# creating the bag of words model\n",
    "for idx, doc in enumerate(x_patterns):\n",
    "    bow = []\n",
    "    text = lemmatizer.lemmatize(doc.lower())\n",
    "    for word in words:\n",
    "        bow.append(1) if word in text else bow.append(0)\n",
    "    # mark the index of class that the current pattern is associated to\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes_inner.index(y_inner_intents[idx])] = 1\n",
    "    # add the one hot encoded BoW and associated classes to training \n",
    "    training.append([bow, output_row])\n",
    "# shuffle the data and convert it to an array\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "# split the features and target labels\n",
    "train_X = np.array(list(training[:, 0]))\n",
    "train_y = np.array(list(training[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some parameters\n",
    "input_shape = (len(train_X[0]),)\n",
    "output_shape = len(train_y[0])\n",
    "epochs = 200\n",
    "# the deep learning model\n",
    "model_inner = Sequential()\n",
    "model_inner.add(Dense(256, input_shape=input_shape, activation=\"relu\"))\n",
    "model_inner.add(Dropout(0.3))\n",
    "model_inner.add(Dense(64, activation=\"relu\"))\n",
    "model_inner.add(Dropout(0.3))\n",
    "model_inner.add(Dense(output_shape, activation = \"softmax\"))\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
    "model_inner.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=[\"accuracy\"])\n",
    "print(model_inner.summary())\n",
    "model_inner.fit(x=train_X, y=train_y, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1fd02",
   "metadata": {},
   "source": [
    "#### Outer Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836f0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for training data\n",
    "training = []\n",
    "out_empty = [0] * len(classes_outer)\n",
    "# creating the bag of words model\n",
    "for idx, doc in enumerate(x_patterns):\n",
    "    bow = []\n",
    "    text = lemmatizer.lemmatize(doc.lower())\n",
    "    for word in words:\n",
    "        bow.append(1) if word in text else bow.append(0)\n",
    "    # mark the index of class that the current pattern is associated to\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes_outer.index(y_outer_intents[idx])] = 1\n",
    "    # add the one hot encoded BoW and associated classes to training \n",
    "    training.append([bow, output_row])\n",
    "# shuffle the data and convert it to an array\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "# split the features and target labels\n",
    "train_X = np.array(list(training[:, 0]))\n",
    "train_y = np.array(list(training[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8341c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining some parameters\n",
    "input_shape = (len(train_X[0]),)\n",
    "output_shape = len(train_y[0])\n",
    "epochs = 200\n",
    "# the deep learning model\n",
    "model_outer = Sequential()\n",
    "model_outer.add(Dense(256, input_shape=input_shape, activation=\"relu\"))\n",
    "model_outer.add(Dropout(0.3))\n",
    "model_outer.add(Dense(64, activation=\"relu\"))\n",
    "model_outer.add(Dropout(0.3))\n",
    "model_outer.add(Dense(output_shape, activation = \"softmax\"))\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.01, decay=1e-6)\n",
    "model_outer.compile(loss='categorical_crossentropy',\n",
    "              optimizer=adam,\n",
    "              metrics=[\"accuracy\"])\n",
    "print(model_outer.summary())\n",
    "model_outer.fit(x=train_X, y=train_y, epochs=200, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860189b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    return tokens\n",
    "\n",
    "def bag_of_words(text, vocab): \n",
    "    tokens = clean_text(text)\n",
    "    bow = [0] * len(vocab)\n",
    "    for w in tokens: \n",
    "        for idx, word in enumerate(vocab):\n",
    "            if word == w: \n",
    "                bow[idx] = 1\n",
    "    return np.array(bow)\n",
    "\n",
    "def pred_inner(text, vocab, labels): \n",
    "    bow = bag_of_words(text, vocab)\n",
    "    result = model_inner.predict(np.array([bow]))[0]\n",
    "    thresh = 0.2\n",
    "    y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
    "\n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append(labels[r[0]])\n",
    "    return return_list\n",
    "\n",
    "def pred_outer(text, vocab, labels): \n",
    "    bow = bag_of_words(text, vocab)\n",
    "    result = model_outer.predict(np.array([bow]))[0]\n",
    "    thresh = 0.2\n",
    "    y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
    "\n",
    "    y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in y_pred:\n",
    "        return_list.append(labels[r[0]])\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1ab00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(inner_intent, outer_intent, intents_json): \n",
    "    response = \"\"\n",
    "    inner_intent_list = intents_json[outer_intent]\n",
    "    for dictionary in inner_intent_list:\n",
    "        if dictionary['tag'] == inner_intent:\n",
    "            response = random.choice(dictionary[\"responses\"])\n",
    "            break\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6548ab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display\n",
    "def lulu_speak():\n",
    "    display(IPython.display.Audio(\"res.mp3\", rate = 44100, autoplay=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88faa91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# message = input(\"\")\n",
    "# inner_intent = pred_inner(message, words, classes_inner)[0]\n",
    "# outer_intent = pred_outer(message, words, classes_outer)[0]\n",
    "# chat = get_response(inner_intent, outer_intent, data)\n",
    "# print(inner_intent,outer_intent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc37d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for speech-to-text\n",
    "import speech_recognition as sr\n",
    "# for text-to-speech\n",
    "from gtts import gTTS\n",
    "# for language model\n",
    "import transformers\n",
    "# for playing audio\n",
    "import IPython\n",
    "# for data\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "# Building the AI\n",
    "class ChatBot():\n",
    "    def __init__(self, name):\n",
    "        print(\"----- Starting up\", name, \"-----\")\n",
    "        self.name = name\n",
    "    \n",
    "    def speech_to_text(self):\n",
    "        recognizer = sr.Recognizer()\n",
    "        with sr.Microphone() as mic:\n",
    "            print(\"Listening...\")\n",
    "            audio = recognizer.listen(mic)\n",
    "            self.text=\"ERROR\"\n",
    "        try:\n",
    "            self.text = recognizer.recognize_google(audio)\n",
    "            print(\"Me  --> \", self.text)\n",
    "            \n",
    "        except:\n",
    "            print(\"Me  -->  ERROR\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def text_to_speech(text):\n",
    "        print(\"Lulu --> \", text)\n",
    "        speaker = gTTS(text=text, lang=\"en\", slow=False)\n",
    "        speaker.save(\"res.mp3\")\n",
    "        statbuf = os.stat(\"res.mp3\")\n",
    "        mbytes = statbuf.st_size / 1024\n",
    "        duration = mbytes / 200\n",
    "#         os.system('start res.mp3')  #if you are using mac->afplay or else for windows->start\n",
    "#         os.system(\"play \"+\"res.mp3\"+\" tempo 1.5\")\n",
    "#         os.system(\"close res.mp3\")\n",
    "        lulu_speak()\n",
    "        time.sleep(int(50*duration))\n",
    "        os.remove(\"res.mp3\")\n",
    "    \n",
    "    def wake_up(self, text):\n",
    "        return True if self.name in text.lower() else False\n",
    "   \n",
    "    @staticmethod\n",
    "    def logging(self, lulu_text, inner_intent, outer_intent):\n",
    "        log_list = [self.text, lulu_text, inner_intent, outer_intent, datetime.now().strftime('%H:%M:%S %p')]\n",
    "        return log_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe576eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Running the AI\n",
    "if __name__ == \"__main__\":\n",
    "    ai = ChatBot(name=\"Lulu the Piggy\")\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "    inner_intent, outer_intent = \"\", \"\"\n",
    "    ex = True\n",
    "    while ex:\n",
    "        ai.speech_to_text()\n",
    "        ## wake up\n",
    "        if ai.wake_up(ai.text) is True:\n",
    "            answer = \"Hello I am Lulu the Piggy, what can I do for you?\"\n",
    "        ## respond politely\n",
    "        elif any(i in ai.text for i in [\"thank\",\"thanks\",\"appreciate\"]):\n",
    "            answer = np.random.choice([\"You're welcome!\",\"Anytime!\",\"No problem!\",\"Cool!\",\"I'm here if you need me!\"])\n",
    "        elif any(i in ai.text for i in [\"exit\",\"bye\",\"close\"]):\n",
    "            answer = np.random.choice([\"Tata\",\"Have a good day\",\"Bye\",\"Goodbye\",\"Hope to meet soon\",\"Peace out!\"])\n",
    "            ex = False\n",
    "        ## conversation\n",
    "        else:   \n",
    "            if ai.text == \"ERROR\":\n",
    "                answer = \"Sorry, come again?\"\n",
    "            else:\n",
    "                ## use the 2 deep neural nets\n",
    "                inner_intent = pred_inner(ai.text, words, classes_inner)[0]\n",
    "                outer_intent = pred_outer(ai.text, words, classes_outer)[0]\n",
    "                if inner_intent == \"GoogleSearch\":\n",
    "                    get_google_1result(ai.text)\n",
    "                    chat = get_google_search(ai.text)\n",
    "                    answer = str(chat)\n",
    "                elif inner_intent == \"MapsSearch\":\n",
    "                    query = keyword_extractor(ai.text)\n",
    "                    location = location_bot.get_1st_location(query)\n",
    "                    html_text = location['photos'][0]['html_attributions'][0]\n",
    "                    soup = BeautifulSoup(html_text, 'html.parser')\n",
    "                    print(\"Photo link: \"+soup.find('a').get('href'))\n",
    "                    if 'establishment' in location['types']:\n",
    "                        answer = \"Name: \"+location['name']\n",
    "                        answer += \"\\nAddress: \"+location['formatted_address']\n",
    "                        answer += \"\\nOpen now: \"+(\"yes.\" if location['opening_hours']['open_now'] else \"no.\")\n",
    "                        answer += \"\\nRating: \"+str(location['rating'])\n",
    "                        try:\n",
    "                            answer += \"\\nPrice level: \"+\"$\"*int(location['price_level'])\n",
    "                        except:\n",
    "                            pass\n",
    "                    else:\n",
    "                        answer = \"Not an establishment, unable to print results.\"\n",
    "                elif inner_intent == \"NewsSearch\":\n",
    "                    query = keyword_extractor(ai.text)\n",
    "                    news = news_bot.get_1st_news(query)\n",
    "                    try:\n",
    "                        print(\"Link: \"+news['link'])\n",
    "                        answer = \"Title: \"+news['title']\n",
    "                    except:\n",
    "                        answer = \"No news available at this time.\"\n",
    "                else:\n",
    "                    chat = get_response(inner_intent, outer_intent, data)\n",
    "                    answer = str(chat)\n",
    "        ## convert answer to speech\n",
    "        if len(answer) > 0:\n",
    "            ai.text_to_speech(answer)\n",
    "        else:\n",
    "            answer = \"Sorry, I don't understand you. Could you say again?\"\n",
    "            ai.text_to_speech(answer)\n",
    "        if outer_intent == \"Goodbye\":\n",
    "            ex = False\n",
    "    print(\"----- Closing down \"+ai.name+\" -----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe485a56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
